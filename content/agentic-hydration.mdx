---
title: Agentic Hydration
description: A Pattern for Efficient and Reliable AI Component Generation
date: "2025-03-18"
image: /blog/agentic-hydration.png
---

## Motivation

At Quail, while developing our AI-powered data visualization tools, we encountered several challenges that are common in LLM-based applications:

- High token usage leading to increased costs
- Slow response times affecting user experience
- Hallucinations where the model generates incorrect outputs

To address these challenges, we adopted an approach we termed Agentic Hydration. 
Although we didn't invent this approach, we're sharing it as a useful framework for developers aiming to build faster, more accurate, and efficient AI systems.

## What is Agentic Hydration?
At a high level, Agentic Hydration is a two-step pattern for component generation:

1. Generate: Leverage the structured properties of your data to reduce the information the model must directly process and generate. 
   This is done by creating a scaffold that describes the component's structure and behavior.
2. Hydrate: Populate the generated scaffold with the actual data. 

The intuition behind this approach is to allow the LLM to remain efficient, focused, and less prone to errors by minimizing its exposure to raw data. 
This significantly improves both the speed and reliability of the final output.

## Example: Chart Generation

To illustrate the Agentic Hydration pattern, let's consider a use case that is relevant to our workflow: generating a chart based on user input and data.
In this example, we will create a simplified chart component that visualizes data based on user queries.
### Traditional Approach

Typically, you'd prompt an LLM to generate the entire chart component at once.
This might look something like this:

```typescript
export async function generateChart(results: any[], userQuery: string) {
  const { chart } = await generateText({
    model: openai('gpt-4o'),
    system: 'You are a data visualization expert.',
    prompt: `Generate a chart for User Query: "${userQuery}" 
    with data points: ${JSON.stringify(results)}. 
    Return JSX code for visualization.`,
  });

  return chart;
}
```

Problems with this approach:
- High token usage: Sending large datasets to the LLM consumes many tokens
- Increased likelihood of hallucinations: The LLM may misinterpret or misrepresent the data
- Increased latency and cost: Processing large datasets takes more time and resources

### Agentic Hydration Approach

Instead, we split the process into two distinct steps:

#### 1. Chart Configuration Generation

First, generate only the metadata needed for visualization decisions:

```typescript
import { generateObject } from 'ai';
import { z } from 'zod';

// Define a schema to constrain and structure the LLM's output
export const configSchema = z.object({
  type: z.enum(['bar', 'line', 'area', 'pie'])
    .describe('Type of chart'),
  xKey: z.string()
    .describe('Key for x-axis'),
  yKeys: z.array(z.string())
    .describe('Keys for y-axis'),
  description: z.string()
    .describe('Brief description of the chart'),
  title: z.string()
    .describe('Title of the chart'),
});

export async function generateChartConfig(results: any[], userQuery: string) {
  // Note: We only send a SAMPLE of the data, not the entire dataset
  const { object: config } = await generateObject({
    model: openai('gpt-4o'),
    system: 'You are a data visualization expert.',
    prompt: `Generate minimal chart config for "${userQuery}" 
      with columns: ${JSON.stringify(results[0])}`,
    schema: configSchema,
  });

  return config;
}
```

#### 2. Chart Rendering with Data Hydration

Then, use the configuration to render the chart with the complete dataset:

```typescript
import { LineChart, Line, XAxis, YAxis, Tooltip } from 'recharts';
import { generateChartConfig } from './actions';

export default async function DynamicChart({ results, userQuery }) {
  // Get the chart configuration with minimal token usage
  const config = await generateChartConfig(results, userQuery);

  // Hydrate the chart by injecting the full dataset
  return (
    <div className="chart-container">
      <h2>{config.title}</h2>
      <h3>{config.description}</h3>
      <LineChart width={500} height={300} data={results}>
        <XAxis dataKey={config.xKey} />
        <YAxis />
        <Tooltip />
        {config.yKeys.map((key, index) => (
          <Line
            key={key}
            dataKey={key}
            stroke={`hsl(var(--chart-${index + 1}))`}
          />
        ))}
      </LineChart>
    </div>
  );
}
```

## Generalized Workflow

Here is a generalized implementation of this pattern:

1. **Identify minimal metadata**: Determine the smallest subset of metadata required for the LLM to accurately generate a UI scaffold.
2. **Generate scaffold**: Use the metadata in a prompt to instruct the LLM to create the initial structure of your component.
3. **Retrieve data**: Independently query or retrieve the specific data needed for your component.
4. **Hydrate and render UI**: Merge the scaffold with your data and render the component

## Benefits & Considerations

We found that implementing the agentic hydration pattern provided several key advantages:

- **Reduced Token Usage**: Because the prompts utilize summarized metadata, the number of tokens required no longer scales linearly with the dataset size, but remains relatively constant.
- **Hallucination-Free Data**: While hallucinations from the model are still possible, injecting the data after LLM generation ensures data accuracy and significantly reduces the risk of incorrect outputs.
- **Decreased Latency**: With fewer tokens required for model inputs and outputs, we observed substantially faster generation times, improving overall responsiveness and user experience.

Despite these benefits, Agentic Hydration isn't a catch-all solution. Important considerations include:

- **Context Requirements**: Some scenarios inherently require the model to have full data context upfront, limiting the effectiveness of this approach.
- **Increased Complexity**: Separating generation and hydration adds extra steps to the development workflow, introducing a trade-off between implementation complexity and efficiency gains.

For these reasons, scenarios that deal with relatively small token inputs or outputs may find that this pattern may not be worth the added complexity.

## Conclusion

Agentic Hydration significantly improved how we build AI-generated UI components at Quail, enhancing our speed, accuracy, and efficiency. We encourage other developers to adapt and experiment with this pattern.